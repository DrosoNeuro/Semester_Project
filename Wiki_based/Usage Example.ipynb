{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example - Localizer()\n",
    "\n",
    "In this notebook we will look at how we can use the localizer class based on wikipedia pages with an example sentence. During the process we will review the different functions of the class. \n",
    "\n",
    "### Needed packages\n",
    "\n",
    "\"pip install wikipedia, pandas, numpy, sklearn\"\n",
    "\n",
    "### Imports\n",
    "\n",
    "Imports are being taken care of in the class directly, thus we only need to import localizer.py to get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import localizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localizations\n",
    "\n",
    "To work with this class you need a list of localizations that will be the places you want to search for using the localizer class. In our case we will work with the sample_localizations givent by the class as default. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localizer class \n",
    "\n",
    "We will now instanciate an empty Localizer class to which we will add the default localizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = localizer.Localizer()\n",
    "L.add_listLocation()\n",
    "# L.add_listLocation(list) could be used if you want to add your own list of localizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_WikiText()\n",
    "\n",
    "By using this function we will go and fetch the content of each localization using the [wikipedia API](https://pypi.python.org/pypi/wikipedia). We will remove all punctuation from the the page. (this process will also be applied to the input sentence, more later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L.get_WikiText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectorizer()\n",
    "\n",
    "This function will apply a [tf-idf](https://en.wikipedia.org/wiki/Tfâ€“idf) using the sklearn function CountVectorizer and using english stop words as default (we could image a multilingual support, the function already takes the language as variable).\n",
    "It will return a vector of tf-idf values and a corresponding vector of features (words). These variables will be also stored in the functions directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L.vectorizer();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make_map(top)\n",
    "\n",
    "This function will get the top n tf-idf scores and return a list of these words in the form of a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L.make_map(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search_for(sentence, top=10)\n",
    "\n",
    "This function will take a sentence and output the top n localizations that have the highest score in relation with that sentence. It first removes all punctuation from the sentence, lowecases everything and then checks if the words are in the top tf-idf words corresponding to the state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alabama', 4), ('Iowa state', 3), ('New York state', 3), ('Rhode Island state', 3), ('South Dakota state', 3), ('Wisconsin state', 3), ('Alaska state', 2), ('Arkansas state', 2), ('California state', 2), ('Connecticut state', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(L.search_for(\"Alabama, my home, my state\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score(sentence, correct_value, top=10)\n",
    "\n",
    "This function will return true of false if the predicted localizations (in the top n) contain the correct localizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(L.score('Alabama, my home, my state', 'Alabama'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible applications\n",
    "\n",
    "The class has been built to have easy to use inputs (strings), one should be carefull with the text encoding in case they want to convert the text from a file into the input. Concerning the analytic usage of the project, the score function can be used top check if the classifier was correct or not. \n",
    "\n",
    "An interesting point of this workflow is that it could be used in any language and/or multilanguage applications since the information is retrieved on wikipedia and the tf-idf could be set to remove the stop words of another language. \n",
    "\n",
    "### Further improvements\n",
    "\n",
    "* Multilanguage support\n",
    "* Non-linear prediction model\n",
    "* Use the tf-idf value to get the score of each state in the search_for function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
